---
title: 'Advantages and Disadvantages of Random Forest in Modeling Species Distribution: From Theory to Practice'
author: "Yuting Vicky Lin"
date: "2021/11/23"

output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
    toc_depth: 2
    toc_float: yes
    
---

***
# Species distribution model (SDM)

The concept of species distribution is deeply connected to the niche theory that begins to be formulated in the early 1990 (Johnson 1910; Grinnell 1917; Elton 1927) and was later innovated by Hutchinson in 1957. He first differentiated the niche into the fundamental and the realized one. The former measures the geographical area where the species can have a positive population growth considering all the abiotic factors, while the latter encompasses not only the abiotic effect but also the competition among species. Therefore, it made the realized niche smaller than the fundamental one. However,  nowadays, we know that some biotic effects such as facilitation (positive interspecific interactions) can help the population positively to grow (Bruno et al. 2003). Since 2007, Soberon integrated the third critical factor, species dispersal or movement capacity, in the niche theory to consider if a species can reach a given area. He argued that before assessing if a particular species can thrive in the geographical area of interest where both abiotic and biotic conditions can satisfy the requirement of species, the species’ ability to arrive at that area must be examined. This is called biotic-abiotic-movement (BAM) concept that incorporates the interplay among these three factors into the assessment of species niche (Figure 1).


<center>

![.](C:/Users/USER.DESKTOP-ELH7F84/OneDrive/R project/Randomforest/Fig3.png){width=40%}
</center>
**Figure 1.** A BAM diagram suggests the three factors affecting the distribution of a species, abiotic factors (A), biotic factors (B), and dispersal or movement ability (M). It is adapted from Soberon (2007). The blue area between A and B indicates the area where the species can potentially have a positive growth considering its abiotic requirement and the prevailing biotic process, or also called “realized niche”. The intersection among A, B, and M (close circle area) represents the area where the species can move in and increase its population


Now, although we know that the SDM originated from the niche theory, the dispute still exists in which niche we are trying to study in SDM, is it the realized niche (the blue area in Figure 1), or the occupied niche (the close circle area in Figure 1), or none of both? There might not be a correct answer for now. Depend on the interest of research, the selected factors can be largely different, resulting in the distinctive niches measured. Indeed, according to the knowledge of the ecologist on the targeted species, the availability of abiotic data, and the sampling scale, the selection of predicting variables in SDM can be very different. Common climate data including in the models are often temperature, precipitation…etc. If your targeted species is the tropical scleractinian corals, factors other than those common ones such as light intensity and sedimentation should also be considered because of the reliance of corals on zooxanthellate. However, those two parameters are much less important in regulating the distribution of pelagic fish species which do not largely rely on light. 

Common mathmatical approach used to fit and predict species distribution including profile techniques, group discriminative techniques, regression-based techniques, and machine learning. Profile techniques interprete the species distribution based only on the presence data, while the group discriminative techniques require not only presence but also absence data to generate SDMs. Example methods for the former is BIOCLIM (Busby 1991), and for the later is Maxent (Phillips et al. 2006). Regression-based techniques such as generalized linear model (GLM) and generalized additive model (GAM) look for the regression between the environmental factors and the species distribution. Lastly, machine learning can improve the performance by itself as it gets more and more data. For example, Artificial neural networks (ANN) and Random forest (RF) are parts of it. Here, I would like to introduce the application of RF on SDMs with a focus on the principle of RF, the advantages and disadvantages on the application, and a demonstration in R.
 


***
# What is a random forest?
Random forest (RF) is a supervised learning algorithm that needs data to train. As its name, RF is an assemble of many individual decision trees which act as classifiers. Each tree can make a decision, and the final decision is made based on the mode (classification tree) or the average (regression tree) from all decision trees. It is commonly used to deal with classification, regression, or prediction tasks and also to measure the relative importance of the input variables. In addition, it is easy to use and the results it produces are usually good and quite straightforward. Combining all these features, it becomes one of the most popular algorithms in machine learning. 


***
# How does it work? 

RF uses a bootstrapped procedure on the dataset to repeatedly create many subsets. Each of the subsets is later used to fit either a classification or a regression tree, and the final decision is made based on the mode or the average from those trees (Figure 2). Subsequently, an optimal model can be obtained by converging many classification and regression trees (CARTs) fitted by the bootstrapped samples. Based on this best model, we can predict the future observations. Here, I will explain step by step how does this algorithm work and terminology should be known to understand this algorithm.

![**Figure 2.** The framework of the Random Forest algorithm](C:/Users/USER.DESKTOP-ELH7F84/OneDrive/R project/Randomforest/Fig1.png)

&nbsp;

## - Steps

Assuming I have a dataset (N) and the independent variables (M) that I think are important features to delineate those observations. By applying RF to this dataset, I can **(1) find out the best model to fit this dataset, (2) predict the future observations according to this model, and (3) calculate the importance of those feature variables.** These three tasks are very helpful for solving the research questions regarding to SDM, and that is probably why RF is attractive to these users. The procedure of this algorithm is:

1.	Build n bootstrap replicates from the dataset N with the equal sampling size (with replacement). For each replicate, a tree will be created. In other words, n is also the number of trees to fit.
2.	Divide each bootstrap replicate into a training and a testing set. The ratio of data in two sets can be 9:1, 8:2, 7:3, or any ratio, there is no fixed rule. It depends on if the performance of the fianl model meet your expectation or not.
3.	For each replicate, fit a tree to the training set. At each node of the tree, a permutation procedure randomly selecting m feature variables from the variable set (M) is conducted to find out the best split based on the Gini Index (see below) that can quantify the within-node entropy. After this tree is successfully made, it would be validated with the testing set. The prediction error of the classification tree is measured by out-of-bag (bootstrapping) error while the one of the regression tree is calculated by mean squared error. 
4.	Asemble n trees to find out the best model.
5.	Based on this model, a prediction can be made for the future observations.

&nbsp;

## - Classification and regression tree (CART) 

CART is a classifier used to sort the observations based on recursive binary decisions. These decisions are made by the given feature variables which have the most information. However, how does the CART decide the binary decisions on each split? Before answering it, we have to know what is a Gini Index. 
Gini Index can measure the probability of a specific variable being incorrectly classified when randomly selected. This index is entropy-based, so their interpretation is similar. Gini Index ranges between 0 to 1, while 0 denotes that all the elements belong to a single class or there is only one class existing (pure), 1 denotes that all the elements randomly distribute among the classes (impure). Mathematically, it is expressed as:

$$
Gini Index=1-\sum_{i = 1}^{n}{(p_i)^2} 
$$
where p~i~ denotes the probability of an element being classified to a specific class.

In each split of a tree, Gini Indeces for each of the variables randomly picked up from its variable set are calculated, and the one with the lowest Gini Index is chosen as the given split (Figure 3). For the part of variables, the more a variable contributes to the prediction process, the more chances it would be retained as the split. While for the observation, the more binary decisions the CART makes, the more similar the feature variables of the subset groups (Figure 3). Nonetheless, more splits on a tree do not necessarily make the model perform better, too many splits may lead to the overfitting of the model.

![**Figure 3.** A CART trying to classify corals based on their morphology and color. The entropy of the dataset decreases each time a split is made.](C:/Users/USER.DESKTOP-ELH7F84/OneDrive/R project/Randomforest/Fig2.png)

&nbsp;

## -	Hyperparameter tuning
Hyperparameter tuning refers to a set of parameters that needs to be decided before starting the algorithm. Those parameters can be adjusted either to make the model perform better or to fasten the model. These are some common parameters that can be set before conducting the RF in R.

1.	Number of trees (ntree)

2.	Number of feature variables randomly sampled as candidates at each split. (mtry)

&nbsp;

## -	Variable importance

An excellent piece of information that we can derive from RF is the relative importance of each feature variable on the predictions.

There are two kinds of measures. The first one measures the mean decrease in the accuracy. It measures the loss of accuracy when excluding the particular variable from the prediction model. Specifically, for each tree, the out-of-bag error (or mean squared error for the regression tree) is computed from its testing data. Then, the error is calculated again after permuting the values of each feature variable in testing data. The difference between the two error values is later averaged over all the trees. After normalization, the value obtained is the decrease in prediction accuracy. 

The second measure is based on the decrease in Gini Index when a variable is selected as a split. In particular, it calculates for each variable the sum of the decrease in the Gini Index from each tree when it is chosen as the split. Then, this sum is divided by the total number of trees built to gain the decrease in the Gini Index of that variable.

&nbsp;



***


# Advantages and disadvantages

After understanding both SDM and RF, we can start to talk about why RF becomes more and more popular in fitting SDM recently. It has several advantages when combining both:

1.	**No need for assumptions before conducting the models** (e.g. independence of variables, normal distribution). This is a very great asset of this non-parametric model, which makes it outcompete other linear-based models (e.g. generalized linear model). One of the required assumptions before conducting the traditional linear-based models is the independence of variables. However, some environmental data are naturally spatially or temporally autocorrelated, additional statistical steps have to be implemented to deal with this issue before starting the linear-based models. Another required assumption when building linear-based models is the normality, however, these environmental data are usually not normally distributed. In this condition, RF is a non-parametric model that can still work when the normality of the data is not meet. 

2.	**Flexibility to address several types of analysis.** As mentioned earlier, RF can address many kinds of statistical analyses or models such as regression, classification, prediction. This flexibility makes RF very useful in modeling species distribution. For example, common types of environmental predictors include categorical and continuous, and the species data can be either presence-absence or abundance. The good news is, RF is able to integrate all of them. 

3.	**Identify non-linear relationships.** The response of species to the environmental condition or change is not always linear, it can be exponential, concave…etc. Sometimes, these non-linear interactions between the environmental factors and species can also be meaningful to affect species distribution. The advantage of RF is that it can uncover non-linear relationships that the linear-based models cannot find. In addition, another limitation when using the linear-based model to fit SDMs is that those linear-based models can only test *a priori* hypothesis and are, therefore, hard to detect the relationship out of this expectation. On the other hand, RF is also able to figure out the complex interactions between environmental variables. For example, they may interact hierarchically with each other. This is because the RF is an assemble of many CARTs, so it can address the environmental factors that interact with hierarchy. 

4.	**Allowing missing value.** RF automatically produces proximity, which is the measure of similarity among data points. This proximity matrix can be used to simulate the missing data that could be obtained when ecologists have to simultaneously sample species and environmental data. 

5.	**Less overfitting.** RF tends to be less overfitting than CART. CART sometimes overfits the data because it would continuously split the nodes to minimize the within-node entropy. However, this issue is overcome in RF by choosing the best CART from many of them through the out-of-bag procedure/averaging many CARTs and also by randomly selecting the candidate variables in each node of trees. Overfitting is the last issue we would like to encounter when constructing a model since it could make the model perform badly in prediction, so does the SDM.


&nbsp;

It seems it is perfect to model the distribution of species with RF. Nonetheless, several disadvantages still exist when conducting it:

1.	**Blackbox approach.** The ecologists have very few controls on what the algorithm does, basically, they can only try with different settings of hyperparameters (see the section of hyperparameter tuning) and different ratios of training and testing sets, then check the model afterward. This blackbox effect can sometimes make the results hard to interpret, however, this may not be a big issue if the ecologists already have a basic understanding of the species they are working on.

2.	**A large number of trees make the algorithm slow to run.** A large enough dataset, as well as number of trees, has to be prepared and built to gain a robust RF model. This process can make the algorithm time-consuming. However, this step is important to avoid the overfitting issue that may occur for the CART mentioned earlier (Point 5 in Advantage). Therefore, the users have to deal with this trade-off between the time running for the RF model and its performance.

***
# Demonstration in R

To demonstrate how to apply RF on SDM in R, I chose three abiotic factors, sea surface temperature (SST), light at the surveyed depth, and chlorophyll *a*, to model the distribution of a scleractinian coral species, *Stylophora pistillata*. The cover data of the coral species from 209 transects sampled in the north, east, and south Taiwan during 2015-2020 were derived from my Ph.D. research project. Each transect sampled a 5.25m^2^ area and one unit value in species cover represent a 0.01 area among 5.25 m^2^. As for the three abiotic parameters, I computed their 2014 mean annual values averaging the 12 months data downloaded from the ERDDP server managed by National Oceanic and Atmospheric Administration (NOAA), USA (dataset ID for temperature is NOAA_DHW, for chlorophyll *a* is erdMH1chlamday, for light at the bottom are erdMH1par0mday and erdMH1kd490mday^1^). Since the abiotic variables I chose are all continuous ones, I will demonstrate the RF with regression trees instead of classification ones.

^1^ light at the surveyed depth = PAR x exp (-kd490 x z) where PAR is photosynthetically available radiation, kd49 means diffuse attenuation k490, and z represents the in situ depth.

&nbsp;

Before starting the analysis, you can download the the datasets (cover of *S. pistillata*, abiotic factors, coordinates of sampling transects, and the three map files of Taiwan), and the R manuscript to follow this RF analysis. 
&nbsp;

```{r , eval = TRUE,  message=F, include=F, warning=F, purl=F, results="hide"}
options(knitr.duplicate.label = "allow")
knitr::purl('index.Rmd', documentation = F)
knitr::purl('Stylophora_pistillata.xlsx', documentation = F)
knitr::purl('cord.xlsx', documentation = F)
knitr::purl('TWN_adm0.shp', documentation = F)
knitr::purl('TWN_adm0.dbf', documentation = F)
knitr::purl('TWN_adm0.shx', documentation = F)
```
```{r , echo=FALSE, purl=F}
xfun::embed_file('Stylophora_pistillata.xlsx')
```
&nbsp;
```{r , echo=FALSE, purl=F}
xfun::embed_file('cord.xlsx')
```
&nbsp;
```{r , echo=FALSE, purl=F}
xfun::embed_file('TWN_adm0.shp')
```
&nbsp;
```{r , echo=FALSE, purl=F}
xfun::embed_file('index.R')
```
</p>


&nbsp;

Package installation 
```{r, include = T, eval= T, results = 'hide', message = F, warning = F} 
# install.packages(c('readxl','caTools','randomForest','rgdal','ggplot2'))
# update.packages()
library(readxl)
library(caTools)
library(randomForest)
library(rgdal)
library(ggplot2)
```

&nbsp;

Data importation
```{r, include = F, eval= T, results = 'hide', message = F} 
setwd('C:\\Users\\USER.DESKTOP-ELH7F84\\OneDrive\\R project\\Randomforest')
```

```{r, include = T, eval= T, results = 'hide', message = F} 
# setwd('') # set your local path
stpi <- read_excel("Stylophora_pistillata.xlsx") # import Stylophora pistillata cover and environmental data from 209 transects
stpi <- stpi[ ,-c(1,5,6)] # remove the column with transect, min SST and max SST information
cord <- read_excel("cord.xlsx") # import the coordinate of sampling transects
taiwan <- readOGR('TWN_adm0.shp') # import the OGR file of Taiwan map 
```

&nbsp;

After importing all the files, let’s start to build the species distribution model with RF. The first thing I do is to divide the whole dataset into training and testing sets with the ratio of 7:3, this ratio can be changed if the new model performed better than this one.
```{r , include = F, set.seed(2)}
knitr::opts_chunk$set(cache = T)
```

```{r, include = T, eval=T} 
sample.stpi <- caTools::sample.split(stpi$SP, SplitRatio = .7)
stpi.train <- subset(stpi, sample.stpi == TRUE)
stpi.test <- subset(stpi, sample.stpi == FALSE)
dim(stpi.train)
dim(stpi.test)
```

&nbsp;

Then, I can start to create the model.


```{r, include = T, eval=T} 
set.seed(17) # set a random seed to make the model reproducible
rf.stpi <- randomForest(SP ~ .,data=stpi.train, importance = T) # build the random forest model
rf.stpi
```

&nbsp;

The result of this model shows that the three factors can together explain 27.84% of the total variance. This is not high, but with only three variables, it is also not bad. Still, more abiotic factors are recommended to add to this model to catch more variance. I use 500 trees (R’s default) to build this model, and caution is needed for the number of trees since too many trees may make the model overfitting. Therefore, checking the mean square errors of models created with the different number of trees is recommended.

* The errors generated each time would be slightly different from this one. This is the outcome I got when I was runnnig this demonstration. Alternatively, you can use the function "set.seed()" to fix the outcome each time you produce so that the result would be reproducible.

```{r,cache.extra = rf.stpi, include = T, eval=T} 
plot(rf.stpi) # check the mean square error of models
```

&nbsp;

From the plot, we know that as more and more trees are built, the errors decrease and eventually become stable. It seems that when the number of trees is around 300-400, we arrived at the lowest error. We can check it with the “which.min” function.

```{r,cache.extra = rf.stpi, include = T, eval=T} 
which.min(rf.stpi$mse)
```

&nbsp;

It turns out that we can have the lowest error in the RF model with 371 trees, so let’s rebuild the model with this number of trees by using the argument “ntree”.

Then, I can start to create the model.
```{r, cache.extra = stpi.train, include = T, eval=T} 
rf.stpi.min <- randomForest(SP ~ .,data=stpi.train, importance = T, ntree=371)
rf.stpi.min
```

&nbsp;

The variance explained slightly increase and the error slightly decrease, so this model performs a little bit better than the one with 500 trees. In addition to this hyperparameter, we can also adjust the number of the variables randomly selected at each split with the argument “mtry” to make the model perform better.
```{r, cache.extra = stpi.train, include = T, eval=T} 
rf.stpi.min.2 <- randomForest(SP ~ .,data=stpi.train, importance = T,ntree=371, mtry=2)
rf.stpi.min.2
rf.stpi.min.3 <- randomForest(SP ~ .,data=stpi.train, importance = T,ntree=371, mtry=3)
rf.stpi.min.3
```

&nbsp;

The models with the number of variables is two does the best since it has the minimum error. The variance explained also increase a bit. Hence, we will use the one with mtry = 2. 

Now, we can check the importance of those variables in this model with the function “varImpPlot” and” importance”.
```{r, include = T, eval=T} 
varImpPlot(rf.stpi.min.2)
importance(rf.stpi.min.2)
```

&nbsp;

The result shows the importance of variables calculated with the mean decrease in accuracy and with the Gini Index. Both suggest that light is the most important factor in driving the distribution of *S. pistillata* followed by SST and the chlorophyll *a*. 

Now, we can try to validate this model with the testing set by calculating the residuals and the mean squared error.
```{r, include = T, eval=T} 
pred.stpi <- predict(rf.stpi.min.2, newdata=stpi.test)
rf.resid.stpi <- pred.stpi - stpi.test$SP # the residuals
plot(stpi.test$SP,rf.resid.stpi, pch=18,ylab="residuals (predicted-observed)",   xlab="observed",col="blue3") # residual plot
abline(h=0,col="red4",lty=2)
mean(rf.resid.stpi^2) # mean squared error
```

&nbsp;

The residual plot shows that the error of the model is around 5-10% when predicting the distribution of *S. pistillata*, and more error would be found in the area with higher cover. This may be because there are too little data in our dataset especially in the geographical area with the higher cover of *S. pistillata*. The value of mean squared error can be used to compare to other models and figure out the best one.

Then, we can have a look on how the individual abiotic variable affecting the model with partial plots with the function ”partialPlot”.
```{r, include = T, eval=T, message = F, results = 'hide'} 
par(mfrow=c(1,3)) 
partialPlot(rf.stpi.min.2, as.data.frame(stpi.train), light)
partialPlot(rf.stpi.min.2, as.data.frame(stpi.train), SST)
partialPlot(rf.stpi.min.2, as.data.frame(stpi.train), Chl_a)
dev.off()
```

&nbsp;

The purpose of the partial plot is to display how the prediction of coral cover is changing with the change of each of the three abiotic variables. If the line is closer to zero, it means that the given variable is not affecting the coral cover at all, while the more the line deviating from zero, this variable influences the coral cover more. Instead of looking at the value on the y-axis, the relationship between the y-axis and the given variable across a specific range is what we should focus on. For example, in the partial dependence plot on light, when the light intensity becomes stronger we have more chances to see a high cover of *S. pistillata*. This increase of chances is even more obvious when the light intensity is between 5 to 15 W/m^2^/day and over 25 W/m^2^/day. Look at the partial dependence plot on SST, we found that the cover of *S. pistillata* would dramatically decrease when the 2014 mean SST is above 26 °C and its cover remains quite consistent when it is below 26 °C. Lastly, for the partial dependence plot on chlorophyll a, it peaks when the concentration of chlorophyll *a* is at around 0.2, 0.4, and 1 mg/m^3^. By checking the tendency and the value in the y-axis on these three partial plots, it may explain why light intensity is the most important factor in predicting the distribution of *S. pistillata*.

Now, let’s also draw the distribution map of *S. pistillata* with cover information to visualize it. 
```{r, include = T, eval=T} 
cord <- cbind(cord,stpi$SP)
colnames(cord) <- c('Tr','Lat',"Lon",'Cover')
lat <- as.factor(cord$Lat)
lon <- as.factor(cord$Lon)
coords <- SpatialPoints(cord[, c("Lon", "Lat")]) 
stpi_cord <- SpatialPointsDataFrame(coords, cord) 
proj4string(stpi_cord) <- CRS("+proj=longlat +no_defs +ellps=WGS84", doCheckCRSArgs = F) 
head(stpi_cord)
class(stpi_cord)

taiwan_map <- fortify(taiwan)
Taiwan.map <- ggplot() +
  geom_polygon(data= taiwan_map, aes(x= long, y= lat, group= group), fill='light grey') +
  scale_x_continuous(limits = c(119, 123)) + # set x and y limt
  scale_y_continuous(limits = c(21.5, 25.5)) +
  theme_minimal()+  # background    
  coord_fixed(1.1)+ # fix the coordinate
  geom_point(cord, mapping = aes(x= Lon, y= Lat, size = Cover),alpha=0.2, color="blue") + # add the location of sampling transects
  theme(legend.position="right")

Taiwan.map+ scale_size_continuous(name = 'Cover (m^2)' , range = c(0, 15),breaks = c(0,5,10,15,20), labels = c('0.01','0.05','0.1','0.15','0.2')) # adjust the size of bubble plot
```

&nbsp;

According to this map, *S. pistillata* has higher cover in the north of Taiwan while the east and the south have less cover. Based on its distribution and the RF model, we can deduce that *S. pistillata* tends to live in the water with higher light intensity and lower SST, concentrating in north Taiwan. The deduction for the light makes sense because this coral species needs light to support the growth of their zooxanthellate, which can in turn provide the coral species energy. Despite tropical scleractinian corals usually favor warm and bright areas, *S. pistillata* is more abundant in the north where the SST is lower. This might be because this species is not competitive enough to other coral species when the thermal environment is suitable for growth, so they are not abundant in the east and the south where the thermal condition can allow many coral species to develop. However, there is another possibility that this dataset may suffer from oversampling in the shallower water of the north (26% of the transects sampled at < 20 meters in the north, 19% of the transects sampled at < 20 meters in the east, 17% of the transects sampled at < 20 meters in the Orchid Island, Green Island and Kenting are less than 17%). It may make the algorithm have more chances to sample the transects in the north when bootstrapping, hence, the results would bias toward the environment of the north shallow water where is colder and lighter. More sampling effort needs to be added to the areas other than the shallow water in the north of Taiwan to make sure if this can be an issue or not. 

Overall, this RF model suggests that the all abiotic variables (SST, light at the surveyed depth, and chlorophyll *a*) positively contribute to the cover of *S. pistillata*. Among these variables, the light is the strongest predictor. With these three factors, we can already catch 25.95% of the total variance. Nonetheless, considering the number of observations and also the number of variables, this dataset is too small to build a very robust RF model. Further sample effort is needed to make this prediction stronger especially in places other than the shallow water in the north of Taiwan. 

&nbsp;

***

# Summary
RF can integrate different types of variables as well as a large amount of data, and seek for the complex relationship between the predicted variables and the response variables that sometimes the linear-based model cannot find. These features allow RF to stand out as one of the most popular approaches in modeling species distribution, in which the complex and dependent relationship commonly exists within and among environmental factors. Furthermore, based on a robust RF model, a prediction on the future alteration of species distribution considering the human-driven changes in any environmental parameters can be further estimated. This information could be quite helpful in conservation. 

&nbsp;

***

# Reference
Bruno JF, Stachowicz JJ, Bertness MD (2003) Inclusion of facilitation into ecological theory. Trends in Ecology and Evolution 18:119-125

Busby JR (1991) BIOCLIM – a bioclimate analysis and prediction system. pp. 64–68 in Margules, C.R. and Austin, M.P. (eds). Nature conservation: cost effective Bbiological surveys and data analysis. CSIRO: Melbourne

Cutler DR, Edwards TC, Beard KH, Catler A, Hess KT, Gibson J, Lawler JJ (2007) Random forest for classification in ecology. Ecology 88:2783–2792 

Elton CS (1927) Animal ecology. Sedgwick & Jackson, London, U.K
Grinnell J (1917) The niche-relationships of the California trasher. The Auk 34:427-433 

Hutchinson GE (1957) Concluding remarks, cold spring harbor symposium. Quantitative Biology 22:415-427 

Jeffrey S. Evans, Melanie A. Murphy, Zachary A. Holden, Cushman SA (2011) Modeling species distribution and change using random forest. In: Drew CA, Wiersma YF, Huettmann F (eds) Predictive species and habitat modeling in landscape ecology. Springer New York Dordrecht Heidelberg London

Johnson RH (1910) Determinate evolution in the color-pattern of the lady-beetles. Carnegie Inst. of Washington, Washington, D.C., USA

Phillips SJ, Anderson RP, Schapire RP (2006) Maximum entropy modeling of species geographic distributions. Ecological Modelling, 190: 231–259

Soberon J (2007) Grinnellian and Eltonian niches and geographic distributions of species. Ecology Letter 10:1115-1123 [10.1111/j.1461-0248.2007.01107.x]

https://builtin.com/data-science/random-forest-algorithm

https://towardsdatascience.com/understanding-random-forest-58381e0602d2

https://damariszurell.github.io/SDM-Intro/#1_Background
